{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://pieriantraining.com/tensorflow-lstm-example-a-beginners-guide/\n",
    "# https://www.tensorflow.org/guide/keras/working_with_rnns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib as plt\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "from keras.models import load_model\n",
    "from keras import layers\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense\n",
    "import joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Text</th>\n",
       "      <th>Source</th>\n",
       "      <th>Human</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ING AsiaPacific Companys Problems Research Pap...</td>\n",
       "      <td>Human</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Crisis Love Inquiry Essay Critical Writing fol...</td>\n",
       "      <td>Human</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Sure sex segregation makes lot sense many spor...</td>\n",
       "      <td>Human</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Christianity Islam Values Essay Christianity f...</td>\n",
       "      <td>Human</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Becca liked swim practiced everyday hours ente...</td>\n",
       "      <td>GLM-130B</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78887</th>\n",
       "      <td>Mass Eoghan Chada 10 brother Ruairi 5 said St ...</td>\n",
       "      <td>OPT-30B</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78888</th>\n",
       "      <td>Asian Teachers Polish Lesson Perfection Stigle...</td>\n",
       "      <td>Human</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78889</th>\n",
       "      <td>Move knife slowly avoid slipping accidentally ...</td>\n",
       "      <td>OPT-6.7B</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78890</th>\n",
       "      <td>Good dreams likely occur person feeling relaxe...</td>\n",
       "      <td>Text-Davinci-003</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78891</th>\n",
       "      <td>Acquired Savant Syndrome Theres 30 cases ever ...</td>\n",
       "      <td>Human</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>78892 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    Text            Source  \\\n",
       "0      ING AsiaPacific Companys Problems Research Pap...             Human   \n",
       "1      Crisis Love Inquiry Essay Critical Writing fol...             Human   \n",
       "2      Sure sex segregation makes lot sense many spor...             Human   \n",
       "3      Christianity Islam Values Essay Christianity f...             Human   \n",
       "4      Becca liked swim practiced everyday hours ente...          GLM-130B   \n",
       "...                                                  ...               ...   \n",
       "78887  Mass Eoghan Chada 10 brother Ruairi 5 said St ...           OPT-30B   \n",
       "78888  Asian Teachers Polish Lesson Perfection Stigle...             Human   \n",
       "78889  Move knife slowly avoid slipping accidentally ...          OPT-6.7B   \n",
       "78890  Good dreams likely occur person feeling relaxe...  Text-Davinci-003   \n",
       "78891  Acquired Savant Syndrome Theres 30 cases ever ...             Human   \n",
       "\n",
       "       Human  \n",
       "0          1  \n",
       "1          1  \n",
       "2          1  \n",
       "3          1  \n",
       "4          0  \n",
       "...      ...  \n",
       "78887      0  \n",
       "78888      1  \n",
       "78889      0  \n",
       "78890      0  \n",
       "78891      1  \n",
       "\n",
       "[78892 rows x 3 columns]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = pd.read_csv('../../data/fraction_preprocessed_data.csv',encoding='ISO-8859-1')\n",
    "dataset = dataset.drop(['Unnamed: 0'], axis=1)\n",
    "dataset = dataset.dropna(subset=['Text'])\n",
    "# dataset = dataset.sample(frac=0.1, random_state=42)\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the tokenizer\n",
    "tokenizer = Tokenizer(num_words=5000, oov_token=\"<OOV>\")\n",
    "tokenizer.fit_on_texts(dataset['Text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert texts to sequences\n",
    "sequences = tokenizer.texts_to_sequences(dataset['Text'])\n",
    "\n",
    "# Pad sequences to ensure uniform input length\n",
    "padded_sequences = pad_sequences(sequences, maxlen=100, padding='post', truncating='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# unitializing label encoder\n",
    "label_encoder = LabelEncoder()\n",
    "label_encoder.fit(dataset['Human'])\n",
    "\n",
    "# labels to numerical format conversion\n",
    "encoded_labels = label_encoder.transform(dataset['Human'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['../../models/NeuralNetworks/LSTM/Variables/padded_sequences.pkl']"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "joblib.dump(tokenizer, '../../models/NeuralNetworks/LSTM/Variables/tokenizer.pkl')\n",
    "joblib.dump(label_encoder, '../../models/NeuralNetworks/LSTM/Variables/label_encoder.pkl')\n",
    "joblib.dump(encoded_labels, '../../models/NeuralNetworks/LSTM/Variables/encoded_labels.pkl')\n",
    "joblib.dump(padded_sequences, '../../models/NeuralNetworks/LSTM/Variables/padded_sequences.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LSTM instantiation\n",
    "lstm = Sequential([\n",
    "    Embedding(input_dim=5000, output_dim=128, input_length=100),\n",
    "    LSTM(128, return_sequences=False),\n",
    "    Dense(64),\n",
    "    Dense(32),\n",
    "    Dense(1, activation='sigmoid')\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1973/1973\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m67s\u001b[0m 33ms/step - accuracy: 0.6753 - loss: 0.5914 - val_accuracy: 0.8042 - val_loss: 0.3980\n",
      "Epoch 2/10\n",
      "\u001b[1m1973/1973\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m65s\u001b[0m 33ms/step - accuracy: 0.8218 - loss: 0.3778 - val_accuracy: 0.8160 - val_loss: 0.3767\n",
      "Epoch 3/10\n",
      "\u001b[1m1973/1973\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m96s\u001b[0m 49ms/step - accuracy: 0.8572 - loss: 0.3103 - val_accuracy: 0.8429 - val_loss: 0.3452\n",
      "Epoch 4/10\n",
      "\u001b[1m1973/1973\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m107s\u001b[0m 54ms/step - accuracy: 0.8945 - loss: 0.2450 - val_accuracy: 0.8418 - val_loss: 0.3631\n",
      "Epoch 5/10\n",
      "\u001b[1m1973/1973\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m63s\u001b[0m 32ms/step - accuracy: 0.9159 - loss: 0.1961 - val_accuracy: 0.8499 - val_loss: 0.3807\n",
      "Epoch 6/10\n",
      "\u001b[1m1973/1973\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m61s\u001b[0m 31ms/step - accuracy: 0.9383 - loss: 0.1512 - val_accuracy: 0.8456 - val_loss: 0.4234\n",
      "Epoch 7/10\n",
      "\u001b[1m1973/1973\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m60s\u001b[0m 30ms/step - accuracy: 0.9559 - loss: 0.1085 - val_accuracy: 0.8414 - val_loss: 0.4810\n",
      "Epoch 8/10\n",
      "\u001b[1m1973/1973\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m59s\u001b[0m 30ms/step - accuracy: 0.9692 - loss: 0.0815 - val_accuracy: 0.8437 - val_loss: 0.5808\n",
      "Epoch 9/10\n",
      "\u001b[1m1973/1973\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m77s\u001b[0m 39ms/step - accuracy: 0.9769 - loss: 0.0634 - val_accuracy: 0.8429 - val_loss: 0.6695\n",
      "Epoch 10/10\n",
      "\u001b[1m1973/1973\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m108s\u001b[0m 55ms/step - accuracy: 0.9834 - loss: 0.0460 - val_accuracy: 0.8439 - val_loss: 0.7141\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x19c6c9669e0>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# training phase\n",
    "lstm.fit(padded_sequences, encoded_labels, epochs=10, batch_size=32, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 0.98"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm.save('../../models/NeuralNetworks/LSTM/lstm_three_98.keras')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
