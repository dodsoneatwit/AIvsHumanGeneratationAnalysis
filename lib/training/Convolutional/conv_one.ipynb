{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://pieriantraining.com/tensorflow-lstm-example-a-beginners-guide/\n",
    "# https://www.tensorflow.org/guide/keras/working_with_rnns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "from keras.models import load_model\n",
    "from keras import layers\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense, Conv1D, GlobalMaxPooling1D\n",
    "import joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Text</th>\n",
       "      <th>Source</th>\n",
       "      <th>Human</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>287452</th>\n",
       "      <td>ING AsiaPacific Companys Problems Research Pap...</td>\n",
       "      <td>Human</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>222199</th>\n",
       "      <td>Crisis Love Inquiry Essay Critical Writing fol...</td>\n",
       "      <td>Human</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>453600</th>\n",
       "      <td>Sure sex segregation makes lot sense many spor...</td>\n",
       "      <td>Human</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>276338</th>\n",
       "      <td>Christianity Islam Values Essay Christianity f...</td>\n",
       "      <td>Human</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78217</th>\n",
       "      <td>Becca liked swim practiced everyday hours ente...</td>\n",
       "      <td>GLM-130B</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>663613</th>\n",
       "      <td>Mass Eoghan Chada 10 brother Ruairi 5 said St ...</td>\n",
       "      <td>OPT-30B</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>285976</th>\n",
       "      <td>Asian Teachers Polish Lesson Perfection Stigle...</td>\n",
       "      <td>Human</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>679335</th>\n",
       "      <td>Move knife slowly avoid slipping accidentally ...</td>\n",
       "      <td>OPT-6.7B</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>775773</th>\n",
       "      <td>Good dreams likely occur person feeling relaxe...</td>\n",
       "      <td>Text-Davinci-003</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>366983</th>\n",
       "      <td>Acquired Savant Syndrome Theres 30 cases ever ...</td>\n",
       "      <td>Human</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>78892 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                     Text            Source  \\\n",
       "287452  ING AsiaPacific Companys Problems Research Pap...             Human   \n",
       "222199  Crisis Love Inquiry Essay Critical Writing fol...             Human   \n",
       "453600  Sure sex segregation makes lot sense many spor...             Human   \n",
       "276338  Christianity Islam Values Essay Christianity f...             Human   \n",
       "78217   Becca liked swim practiced everyday hours ente...          GLM-130B   \n",
       "...                                                   ...               ...   \n",
       "663613  Mass Eoghan Chada 10 brother Ruairi 5 said St ...           OPT-30B   \n",
       "285976  Asian Teachers Polish Lesson Perfection Stigle...             Human   \n",
       "679335  Move knife slowly avoid slipping accidentally ...          OPT-6.7B   \n",
       "775773  Good dreams likely occur person feeling relaxe...  Text-Davinci-003   \n",
       "366983  Acquired Savant Syndrome Theres 30 cases ever ...             Human   \n",
       "\n",
       "        Human  \n",
       "287452      1  \n",
       "222199      1  \n",
       "453600      1  \n",
       "276338      1  \n",
       "78217       0  \n",
       "...       ...  \n",
       "663613      0  \n",
       "285976      1  \n",
       "679335      0  \n",
       "775773      0  \n",
       "366983      1  \n",
       "\n",
       "[78892 rows x 3 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = pd.read_csv('../../data/preprocessed_data.csv',encoding='ISO-8859-1')\n",
    "dataset = dataset.drop(['Unnamed: 0'], axis=1)\n",
    "dataset = dataset.dropna(subset=['Text'])\n",
    "dataset = dataset.sample(frac=0.1, random_state=42)\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the tokenizer\n",
    "tokenizer = Tokenizer(num_words=5000, oov_token=\"<OOV>\")\n",
    "tokenizer.fit_on_texts(dataset['Text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert texts to sequences\n",
    "sequences = tokenizer.texts_to_sequences(dataset['Text'])\n",
    "\n",
    "# Pad sequences to ensure uniform input length\n",
    "padded_sequences = pad_sequences(sequences, maxlen=100, padding='post', truncating='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# unitializing label encoder\n",
    "label_encoder = LabelEncoder()\n",
    "label_encoder.fit(dataset['Human'])\n",
    "\n",
    "# labels to numerical format conversion\n",
    "encoded_labels = label_encoder.transform(dataset['Human'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convolutional instantiation 1 Conv Layer\n",
    "conv = Sequential([\n",
    "    Embedding(input_dim=5000, output_dim=128, input_length=100),\n",
    "    Conv1D(128, 1, activation='relu'),\n",
    "    GlobalMaxPooling1D(),\n",
    "    Dense(1, activation='sigmoid')\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 1]\n"
     ]
    }
   ],
   "source": [
    "print(np.unique(encoded_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "conv.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001b[1m1973/1973\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 5ms/step - accuracy: 0.7519 - loss: 0.4838 - val_accuracy: 0.8069 - val_loss: 0.3874\n",
      "Epoch 2/10\n",
      "\u001b[1m1973/1973\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 4ms/step - accuracy: 0.8352 - loss: 0.3495 - val_accuracy: 0.8185 - val_loss: 0.3719\n",
      "Epoch 3/10\n",
      "\u001b[1m1973/1973\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 4ms/step - accuracy: 0.8620 - loss: 0.3029 - val_accuracy: 0.8234 - val_loss: 0.3731\n",
      "Epoch 4/10\n",
      "\u001b[1m1973/1973\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 4ms/step - accuracy: 0.8871 - loss: 0.2571 - val_accuracy: 0.8270 - val_loss: 0.3770\n",
      "Epoch 5/10\n",
      "\u001b[1m1973/1973\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 4ms/step - accuracy: 0.9117 - loss: 0.2145 - val_accuracy: 0.8249 - val_loss: 0.3983\n",
      "Epoch 6/10\n",
      "\u001b[1m1973/1973\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 5ms/step - accuracy: 0.9315 - loss: 0.1786 - val_accuracy: 0.8233 - val_loss: 0.4246\n",
      "Epoch 7/10\n",
      "\u001b[1m1973/1973\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 4ms/step - accuracy: 0.9475 - loss: 0.1436 - val_accuracy: 0.8195 - val_loss: 0.4577\n",
      "Epoch 8/10\n",
      "\u001b[1m1973/1973\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 4ms/step - accuracy: 0.9612 - loss: 0.1124 - val_accuracy: 0.8187 - val_loss: 0.4954\n",
      "Epoch 9/10\n",
      "\u001b[1m1973/1973\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 4ms/step - accuracy: 0.9724 - loss: 0.0866 - val_accuracy: 0.8179 - val_loss: 0.5470\n",
      "Epoch 10/10\n",
      "\u001b[1m1973/1973\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 4ms/step - accuracy: 0.9791 - loss: 0.0682 - val_accuracy: 0.8221 - val_loss: 0.5916\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x1f9c7db1630>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# training phase\n",
    "conv.fit(padded_sequences, encoded_labels, epochs=10, batch_size=32, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 0.9843"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conv.save('../../models/NeuralNetworks/conv_one_98.keras')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
